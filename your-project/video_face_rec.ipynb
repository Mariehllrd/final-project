{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial recognition on video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input movie file\n",
    "video = cv2.VideoCapture(\"./data/trump.mp4\")\n",
    "length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter('./result/result.mp4', fourcc, 20.0, (640,  480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and encode Trump image\n",
    "trump_image = cv2.imread(\"./data/donald_trump.jpg\")\n",
    "trump_encoding = face_recognition.face_encodings(trump_image)[0]\n",
    "\n",
    "known_face_encodings = [trump_encoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "frame_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: dlib.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),uint8], face: dlib.full_object_detection, num_jitters: int=0, padding: float=0.25) -> dlib.vector\n    2. (self: dlib.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),uint8], num_jitters: int=0) -> dlib.vector\n    3. (self: dlib.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),uint8], faces: dlib.full_object_detections, num_jitters: int=0, padding: float=0.25) -> dlib.vectors\n    4. (self: dlib.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),uint8]], batch_faces: List[dlib.full_object_detections], num_jitters: int=0, padding: float=0.25) -> dlib.vectorss\n    5. (self: dlib.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),uint8]], num_jitters: int=0) -> dlib.vectors\n\nInvoked with: <dlib.face_recognition_model_v1 object at 0x1125bee70>, array([[ 99, 121, 110, ...,  24,  24,  24],\n       [123, 145, 135, ...,  25,  25,  25],\n       [122, 144, 134, ...,  26,  26,  26],\n       ...,\n       [ 66,  80,  79, ...,  30,  32,  27],\n       [ 68,  82,  81, ...,  35,  37,  31],\n       [ 56,  70,  68, ...,  31,  33,  28]], dtype=uint8), <dlib.full_object_detection object at 0x129f89370>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ca4bbf7f770b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Find and encode faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mface_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_encodings\u001b[0;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \"\"\"\n\u001b[1;32m    213\u001b[0m     \u001b[0mraw_landmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_face_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_landmark_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jitters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mraw_landmark_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_landmarks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \"\"\"\n\u001b[1;32m    213\u001b[0m     \u001b[0mraw_landmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_face_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_landmark_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jitters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mraw_landmark_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_landmarks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: dlib.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),uint8], face: dlib.full_object_detection, num_jitters: int=0, padding: float=0.25) -> dlib.vector\n    2. (self: dlib.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),uint8], num_jitters: int=0) -> dlib.vector\n    3. (self: dlib.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),uint8], faces: dlib.full_object_detections, num_jitters: int=0, padding: float=0.25) -> dlib.vectors\n    4. (self: dlib.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),uint8]], batch_faces: List[dlib.full_object_detections], num_jitters: int=0, padding: float=0.25) -> dlib.vectorss\n    5. (self: dlib.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),uint8]], num_jitters: int=0) -> dlib.vectors\n\nInvoked with: <dlib.face_recognition_model_v1 object at 0x1125bee70>, array([[ 99, 121, 110, ...,  24,  24,  24],\n       [123, 145, 135, ...,  25,  25,  25],\n       [122, 144, 134, ...,  26,  26,  26],\n       ...,\n       [ 66,  80,  79, ...,  30,  32,  27],\n       [ 68,  82,  81, ...,  35,  37,  31],\n       [ 56,  70,  68, ...,  31,  33,  28]], dtype=uint8), <dlib.full_object_detection object at 0x129f89370>, 1"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Frame by frame\n",
    "    ret, frame = video.read()\n",
    "    frame_number += 1\n",
    "\n",
    "    # Break when the video file ends\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Find and encode faces\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "\n",
    "    # Matches\n",
    "    \n",
    "    face_names = []\n",
    "\n",
    "    for face_encoding in face_encodings:\n",
    "        match = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.50)\n",
    "\n",
    "        name = None\n",
    "        if match[0]:\n",
    "            name = \"Trump\"\n",
    "\n",
    "        face_names.append(name)\n",
    "\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 1)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 12), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left, bottom - 4), font, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "    # Save output\n",
    "    print(\"Writing frame {} / {}\".format(frame_number, length))\n",
    "    out.write(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trump_finder(video_path):\n",
    "    # Import and encode Trump image\n",
    "    trump_image = cv2.imread(\"./data/donald_trump.jpg\")\n",
    "    trump_encoding = face_recognition.face_encodings(trump_image)[0]\n",
    "\n",
    "    known_face_encodings = [trump_encoding]\n",
    "\n",
    "    # Load movie file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter('./result/result.avi', fourcc, 20.0, (640,  480))\n",
    "    \n",
    "    face_locations = []\n",
    "    face_encodings = []\n",
    "    face_names = []\n",
    "    frame_number = 0\n",
    "    \n",
    "    while True:\n",
    "        # Frame by frame\n",
    "        ret, frame = video.read()\n",
    "        frame_number += 1\n",
    "\n",
    "        # Break when the video file ends\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Find and encode faces\n",
    "        face_locations = face_recognition.face_locations(frame)\n",
    "        face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "\n",
    "        # Matches\n",
    "\n",
    "        face_names = []\n",
    "\n",
    "        for face_encoding in face_encodings:\n",
    "            match = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.50)\n",
    "\n",
    "            name = None\n",
    "            if match[0]:\n",
    "                name = \"Trump\"\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            if not name:\n",
    "                continue\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 1)\n",
    "\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(frame, (left, bottom - 12), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, name, (left, bottom - 4), font, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "        # Save output\n",
    "        print(\"Writing frame {} / {}\".format(frame_number, length))\n",
    "        out.write(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
